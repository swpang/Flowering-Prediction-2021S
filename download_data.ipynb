{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testOpenAPI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTCAQQElNucY",
        "outputId": "9e7e8b84-48bf-496e-dfc5-79e380fda4e9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)          "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLBlVCABLdy7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import requests, bs4\n",
        "from lxml import html\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from urllib.request import Request, urlopen, urlretrieve\n",
        "from urllib.parse import urlencode, quote_plus, unquote, urlparse\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW8aAyk9IeSr"
      },
      "source": [
        "path = '/content/drive/MyDrive'\n",
        "data_dir = path + '/DATA/weatherData'\n",
        "url = \"http://apis.data.go.kr/1360000/AsosDalyInfoService/getWthrDataList?\"\n",
        "key = unquote(\"fd7yiWzReD%2F9h0kE9KeIxpJRgIW7WoyTyr10Yov4chDZCooL53NoY%2BJQm1fXPMsD0BvNZkwOBbVdE%2FEbovhqww%3D%3D\")\n",
        "\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "df_stations = pd.read_csv(path + '/DATA/stations_info.csv', header=0, index_col=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3ZyuMjpOI74"
      },
      "source": [
        "def preprocess_file(stnId):\n",
        "    df_data = pd.read_csv(path + '/DATA/{}_data.csv'.format(stnId), header=0, index_col=0)\n",
        "    np_data = np.zeros((1,4))\n",
        "    for i, rows in df_data.iterrows():  \n",
        "        split_date = rows['date'].split('-')\n",
        "        dt = datetime(int(split_date[0]), int(split_date[1]), int(split_date[2]))\n",
        "        jd = '%03d' % (dt.timetuple().tm_yday)\n",
        "        jd = int(jd)\n",
        "        split_date.append(jd)\n",
        "        np_split_date = np.array(split_date).reshape(1,-1)\n",
        "        np_data = np.vstack((np_data, np_split_date))\n",
        "    np_data = np.delete(np_data, 0, 0)\n",
        "\n",
        "    df_data['year'] = np_data[:,0]\n",
        "    df_data['month'] = np_data[:,1]\n",
        "    df_data['day'] = np_data[:,2]\n",
        "    df_data['jday'] = np_data[:,3]\n",
        "\n",
        "    df_data = df_data.reindex(columns=['year', 'month', 'day', 'jday',  \n",
        "                                                        'avgTa', 'minTa', 'maxTa', \n",
        "                                                        'sumRn', 'avgWs', 'minRhm', \n",
        "                                                        'avgRhm', 'sumGsr', 'sumSsHr', \n",
        "                                                        'avgTs', 'avgCm5Te', 'avgCm10Te', \n",
        "                                                        'avgCm20Te', 'avgCm30Te', \n",
        "                                                        'avgM05Te', 'avgM10Te', 'avgM15Te', \n",
        "                                                        'avgM30Te', 'avgM50Te'])\n",
        "    return df_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbAsh7s7TKn7"
      },
      "source": [
        "for stnId in df_stations['stnId']:\n",
        "    if int(stnId) > 152:\n",
        "        df_result = preprocess_file(stnId)\n",
        "        df_result.to_csv(data_dir + '/{}_data.csv'.format(stnId))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMvOIgFRtgdR"
      },
      "source": [
        "def get_url(start_date, end_date, stnID, pageNo):\n",
        "    queryParams = urlencode(\n",
        "            {\n",
        "                quote_plus('serviceKey'):key,\n",
        "                quote_plus('numOfRows'):999,\n",
        "                quote_plus('pageNo'):pageNo,\n",
        "                quote_plus('dataType'):'XML',\n",
        "                quote_plus('dataCd'):'ASOS',\n",
        "                quote_plus('dateCd'):'DAY',\n",
        "                quote_plus('startDt'):start_date,\n",
        "                quote_plus('endDt'):end_date,\n",
        "                quote_plus('stnIds'):stnID\n",
        "            }, encoding='UTF-8', doseq=True\n",
        "        )\n",
        "    return url + queryParams\n",
        "\n",
        "\n",
        "def read_page(start_date, end_date, stnID, pageNo):\n",
        "    targetURL = get_url(start_date, end_date, stnID, pageNo)\n",
        "    print('The query URL is:{}'.format(targetURL))\n",
        "    while True:\n",
        "        try:\n",
        "            response = requests.get(targetURL).text.encode('utf-8')\n",
        "        except requests.exceptions.ConnectionError as e:\n",
        "            print('Connection Error : {}'.format(e))\n",
        "            continue\n",
        "        except requests.exceptions.ChunkedEncodingError as e:\n",
        "            print('Chunked Encoding Error : {}'.format(e))\n",
        "            continue\n",
        "        break\n",
        "\n",
        "    xmlobj = bs4.BeautifulSoup(response, 'lxml-xml')\n",
        "    return xmlobj\n",
        "\n",
        "\n",
        "def find_items(xmlobj, item):\n",
        "    rows = xmlobj.findAll(item)\n",
        "    temp = []\n",
        "    for i in rows:\n",
        "        if i.text == '':\n",
        "            temp.append(np.nan)\n",
        "        else:\n",
        "            temp.append(float(i.text))\n",
        "            continue\n",
        "    np_temp = np.array(temp).reshape(-1,1)\n",
        "    return np_temp\n",
        "\n",
        "\n",
        "def get_data(start_year, stnID):\n",
        "    start_date = str(start_year) + '0101'\n",
        "    end_date = '20210701'\n",
        "    xml_test = read_page(start_date, end_date, stnID, 1)\n",
        "    total_rows = int(xml_test.find('totalCount').text)\n",
        "    page_nums = total_rows // 999 + 1\n",
        "    np_result = np.zeros((1,20))\n",
        "    for i in range(page_nums):\n",
        "        pageNo = i + 1\n",
        "        xml_temp = read_page(start_date, end_date, str(stnID), pageNo)\n",
        "        rows = xml_temp.findAll('tm')\n",
        "        dates = []\n",
        "        for j in rows:\n",
        "            dates.append(j.text)\n",
        "        np_dates = np.array(dates).reshape(-1, 1)\n",
        "        avgTa = find_items(xml_temp, 'avgTa')\n",
        "        minTa = find_items(xml_temp, 'minTa')\n",
        "        maxTa = find_items(xml_temp, 'maxTa')\n",
        "        sumRn = find_items(xml_temp, 'sumRn')\n",
        "        avgWs = find_items(xml_temp, 'avgWs')\n",
        "        minRhm = find_items(xml_temp, 'minRhm')\n",
        "        avgRhm = find_items(xml_temp, 'avgRhm')\n",
        "        sumGsr = find_items(xml_temp, 'sumGsr')\n",
        "        sumSsHr = find_items(xml_temp, 'sumSsHr')\n",
        "        avgTs = find_items(xml_temp, 'avgTs')\n",
        "        avgCm5Te = find_items(xml_temp, 'avgCm5Te')\n",
        "        avgCm10Te = find_items(xml_temp, 'avgCm10Te')\n",
        "        avgCm20Te = find_items(xml_temp, 'avgCm20Te')\n",
        "        avgCm30Te = find_items(xml_temp, 'avgCm30Te')\n",
        "        avgM05Te = find_items(xml_temp, 'avgM05Te')\n",
        "        avgM10Te = find_items(xml_temp, 'avgM10Te')\n",
        "        avgM15Te = find_items(xml_temp, 'avgM15Te')\n",
        "        avgM30Te = find_items(xml_temp, 'avgM30Te')\n",
        "        avgM50Te = find_items(xml_temp, 'avgM50Te')\n",
        "        np_temp = np.hstack((np_dates, avgTa, minTa, maxTa, sumRn, avgWs, minRhm, avgRhm, sumGsr,\n",
        "                             sumSsHr, avgTs, avgCm5Te, avgCm10Te, avgCm20Te, avgCm30Te, avgM05Te,\n",
        "                             avgM10Te, avgM15Te, avgM30Te, avgM50Te))\n",
        "        np_result = np.vstack((np_result, np_temp))\n",
        "        print('Intermediate Checkpoint :')\n",
        "        print(np_result[-1,:])\n",
        "    np_result = np.delete(np_result, 0, 0)\n",
        "    return np_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K94XWYHPIrUb"
      },
      "source": [
        "for stn in df_stations['stnId']:\n",
        "    if stn >= 0:\n",
        "        print('Station : {}'.format(stn))\n",
        "        np_data = get_data(1920, int(stn))\n",
        "        df_data = pd.DataFrame(np_data, index=None, columns=['date', 'avgTa', 'minTa', 'maxTa', 'sumRn', 'avgWs',\n",
        "                                                             'minRhm', 'avgRhm', 'sumGsr', 'sumSsHr', 'avgTs',\n",
        "                                                             'avgCm5Te', 'avgCm10Te', 'avgCm20Te', 'avgCm30Te',\n",
        "                                                             'avgM05Te', 'avgM10Te', 'avgM15Te', 'avgM30Te',\n",
        "                                                             'avgM50Te'])\n",
        "        df_data.to_csv('/content/drive/MyDrive/{}_data.csv'.format(stn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xciH0kbGsJd-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}