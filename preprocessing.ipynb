{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KzTXlJeXsBDZ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR1jwKB-qe7w"
      },
      "source": [
        "# 벚꽃 개화시기를 예측해봅시다\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzTXlJeXsBDZ"
      },
      "source": [
        "# Before We Begin..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o8hA2KgbXVE",
        "outputId": "c60e4310-3bfe-489b-ac1e-75ad0711fad9"
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "%config InlineBackend.figure_format = 'retina'\n",
        " \n",
        "!apt -qq -y install fonts-nanum\n",
        "\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        " \n",
        "import matplotlib.font_manager as fm\n",
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "font = fm.FontProperties(fname=fontpath, size=9)\n",
        "plt.rc('font', family='NanumBarunGothic')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 9,604 kB of archives.\n",
            "After this operation, 29.5 MB of additional disk space will be used.\n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 148489 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20170925-1_all.deb ...\n",
            "Unpacking fonts-nanum (20170925-1) ...\n",
            "Setting up fonts-nanum (20170925-1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "fonts-nanum is already the newest version (20170925-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eYpk3NBcvhY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from urllib.request import Request, urlopen, urlretrieve\n",
        "from urllib.parse import urlencode, quote_plus, unquote, urlparse\n",
        "import os\n",
        "from datetime import datetime\n",
        "import statistics\n",
        "from scipy.stats.stats import pearsonr\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn import datasets, linear_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l6pr5gidQZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "953668b5-bde2-406a-b938-9e49a76cdacf"
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive', force_remount=True)          "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNU1P-fnrRKR"
      },
      "source": [
        "# Loading Data for Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6EAJPi6eQ81"
      },
      "source": [
        "## Read Flowering CSV files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyCJq-Evcwqe"
      },
      "source": [
        "path = '/content/drive/MyDrive/DATA/'\n",
        "headers = ['code', 'lat', 'lon', 'year', 'date']\n",
        "\n",
        "df_g = pd.read_csv(path + 'season_g.csv', header=None, names=headers, index_col=None)\n",
        "df_f = pd.read_csv(path + 'season_f.csv', header=None, names=headers, index_col=None)\n",
        "df_fb = pd.read_csv(path + 'season_fb.csv', header=None, names=headers, index_col=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br4X8Rbviz4W"
      },
      "source": [
        "## Read Station Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuIQFSgaeg8y"
      },
      "source": [
        "germ_codes = df_g['code'].values.tolist()\n",
        "flow_codes = df_f['code'].values.tolist()\n",
        "fullb_codes = df_fb['code'].values.tolist()\n",
        "\n",
        "def get_codes(codes):\n",
        "    temp_codes = []\n",
        "    for i in range(len(codes)):\n",
        "        if i == 0:\n",
        "            temp_codes.append(codes[i])\n",
        "        else:\n",
        "            if codes[i] != temp_codes[-1]:\n",
        "                temp_codes.append(codes[i])\n",
        "    return temp_codes\n",
        "\n",
        "# Unique Observatory Codes for each dataset\n",
        "u_germ_codes = get_codes(germ_codes)\n",
        "u_flow_codes = get_codes(flow_codes)\n",
        "u_fullb_codes = get_codes(fullb_codes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGWbrLvVi4AE"
      },
      "source": [
        "df_stations = pd.read_csv(path + 'ASOS_stations.csv', header=0, index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWNSaf9Y7q_M"
      },
      "source": [
        "# Save data of observation station gps coordinates\n",
        "\n",
        "gps_array = np.zeros((len(u_flow_codes), 3))\n",
        "gps_array[:,0] = u_flow_codes\n",
        "\n",
        "for i, code in enumerate(u_flow_codes):\n",
        "    for x, row in df_f.iterrows():\n",
        "        if row['code'] == code:\n",
        "            gps_array[i, 1] = row['lat']\n",
        "            gps_array[i, 2] = row['lon']\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsSIMux_V6eo"
      },
      "source": [
        "## Read Temperature CSV Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vNofoSTV6P8"
      },
      "source": [
        "def read_csv_temp_year(stnId, year):\n",
        "    filepath = '/{}/'.format(stnId)\n",
        "    filename = '{}_{}.csv'.format(year, stnId)\n",
        "    df = pd.read_csv(path + filepath + filename, sep=',', header=0, index_col=None, usecols=[1,2,3,4])\n",
        "    return df\n",
        "\n",
        "def read_csv_temp(stnId):\n",
        "    np_result = np.zeros((1, 4))\n",
        "    filepath = '/{}/'.format(stnId)\n",
        "    foldername = '{}'.format(stnId)\n",
        "    if foldername in os.listdir(path):\n",
        "        for i in np.arange(1961, 2023, 2):\n",
        "            filename = '{}_{}.csv'.format(i, stnId)\n",
        "            if filename in os.listdir(path+filepath):\n",
        "                df_temp = read_csv_temp_year(stnId, i)\n",
        "                #print(df_temp.head())\n",
        "                np_temp = df_temp.to_numpy()\n",
        "                np_result = np.vstack((np_result, np_temp))\n",
        "    np_result = np.delete(np_result, (0), axis=0)\n",
        "    df = pd.DataFrame(np_result, columns=['date', 'avgT', 'minT', 'maxT'])\n",
        "    df = df.dropna(subset=['avgT', 'minT', 'maxT'])\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1cqKQ5eitCx"
      },
      "source": [
        "# Get all data from all stations\n",
        "def read_date(datestring):\n",
        "    temp_string = datestring.split('-')\n",
        "    if len(temp_string) == 3:\n",
        "        for i, item in enumerate(temp_string):\n",
        "            temp_string[i] = int(item)\n",
        "    else:\n",
        "        temp_string = datestring.split('/')\n",
        "        reverse_temp_string = temp_string\n",
        "        for i, item in enumerate(temp_string):\n",
        "            if i < 2:\n",
        "                reverse_temp_string[i+1] = int(item)\n",
        "            else:\n",
        "                reverse_temp_string[-1] = int(item)\n",
        "        temp_string = reverse_temp_string\n",
        "    return temp_string\n",
        "\n",
        "def preprocess_temp_date(temparray):\n",
        "    resultarray = temparray\n",
        "    date_array = np.empty((len(temparray[:,0]), 4))\n",
        "    for i, date in enumerate(temparray[:, 0]):\n",
        "        temp_date = read_date(date)\n",
        "        dt = datetime(int(temp_date[0]), int(temp_date[1]), int(temp_date[2]))\n",
        "        jd = '%03d' % (dt.timetuple().tm_yday)\n",
        "        jd  = int(jd)\n",
        "        temp_date.append(jd)\n",
        "        date_array[i, :] = temp_date\n",
        "    resultarray = np.delete(resultarray, 0, axis=1)\n",
        "    resultarray = np.hstack((date_array[:, 3].reshape(-1, 1), resultarray))\n",
        "    resultarray = np.hstack((date_array[:, 2].reshape(-1, 1), resultarray))\n",
        "    resultarray = np.hstack((date_array[:, 1].reshape(-1, 1), resultarray))\n",
        "    resultarray = np.hstack((date_array[:, 0].reshape(-1, 1), resultarray))\n",
        "    return resultarray\n",
        "\n",
        "data_dict = {}\n",
        "stations_not_counted = []\n",
        "print('Reading Temperature Data')\n",
        "for i, rows in df_stations.iterrows():\n",
        "    df_temp = read_csv_temp(stnId=i)\n",
        "    if not df_temp.empty:\n",
        "        temp_array = df_temp.to_numpy()\n",
        "        print('station : {}'.format(i))\n",
        "        data_dict['{}'.format(i)] = preprocess_temp_date(temp_array)\n",
        "    else:\n",
        "        stations_not_counted.append(i) # removing empty stations\n",
        "\n",
        "# data_dict\n",
        "# year  month   day     jday    avgT    minT    maxT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkxWVPYSgIsY"
      },
      "source": [
        "## Read Seoul (108) Station Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBD3LHGlrffE"
      },
      "source": [
        "## Processing Data for Easy Manipulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBgeWwqZfG37"
      },
      "source": [
        "def preprocess_graph(codes, df):\n",
        "    temp_array = np.zeros((len(range(1920,2022)), len(codes)+1))\n",
        "    temp_array[:,0] = range(1920,2022)\n",
        "\n",
        "    for i, code in enumerate(codes):\n",
        "        for x, row in df.iterrows():\n",
        "            if row['code'] == code:\n",
        "                temp_array[int(row['year'])-1920, i+1] = row['date']\n",
        "\n",
        "    for i in range(len(temp_array[0,:])):\n",
        "        for j in range(len(temp_array[:,0])):\n",
        "            if temp_array[j, i] <= 10:\n",
        "                temp_array[j,i] = np.nan\n",
        "    return temp_array\n",
        "\n",
        "germ_array = preprocess_graph(u_germ_codes, df_g)\n",
        "flow_array = preprocess_graph(u_flow_codes, df_f)\n",
        "fullb_array = preprocess_graph(u_fullb_codes, df_fb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeEgMlFqjYqt"
      },
      "source": [
        "### Differences Between Dates Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko9K-kp1oh5C"
      },
      "source": [
        "flow_germ = []\n",
        "fullb_germ = []\n",
        "fullb_flow = []\n",
        "\n",
        "for i, date in enumerate(seoul_germ):\n",
        "    if date >= 1:\n",
        "        if seoul_flow[i] >= 1:\n",
        "            flow_germ.append(seoul_flow[i] - date)\n",
        "        else:\n",
        "            flow_germ.append(np.nan)\n",
        "    else:\n",
        "        flow_germ.append(np.nan)\n",
        "for i, date in enumerate(seoul_germ):\n",
        "    if date >= 1:\n",
        "        if seoul_fullb[i] >= 1:\n",
        "            fullb_germ.append(seoul_fullb[i] - date)\n",
        "        else:\n",
        "            fullb_germ.append(np.nan)\n",
        "    else:\n",
        "        fullb_germ.append(np.nan)\n",
        "for i, date in enumerate(seoul_flow):\n",
        "    if date >= 1:\n",
        "        if seoul_fullb[i] >= 1:\n",
        "            fullb_flow.append(seoul_fullb[i] - date)\n",
        "        else:\n",
        "            fullb_flow.append(np.nan)\n",
        "    else:\n",
        "        fullb_flow.append(np.nan)\n",
        "\n",
        "fig, axs = plt.subplots(nrows=3, ncols=1, figsize=(10,12), sharex=False, sharey=True)\n",
        "fig.suptitle('Differences Between Dates')\n",
        "\n",
        "axs[0].plot(germ_array[:,0], flow_germ, color='black')\n",
        "sum = 0\n",
        "for i in flow_germ:\n",
        "    if i > 0:\n",
        "        sum += i\n",
        "avg0 = sum / len(flow_germ)\n",
        "axs[0].plot(germ_array[:,0], [avg0] * len(flow_germ), color='black', linestyle='dashed')\n",
        "axs[1].plot(germ_array[:,0], fullb_germ, color='black')\n",
        "sum = 0\n",
        "for i in fullb_germ:\n",
        "    if i > 0:\n",
        "        sum += i\n",
        "avg1 = sum / len(fullb_germ)\n",
        "axs[1].plot(germ_array[:,0], [avg1] * len(flow_germ), color='black', linestyle='dashed')\n",
        "axs[2].plot(germ_array[:,0], fullb_flow, color='black')\n",
        "sum = 0\n",
        "for i in fullb_flow:\n",
        "    if i > 0:\n",
        "        sum += i\n",
        "avg2 = sum / len(fullb_flow)\n",
        "axs[2].plot(germ_array[:,0], [avg2] * len(flow_germ), color='black', linestyle='dashed')\n",
        "\n",
        "axs[0].title.set_text('Flowering - Germination, avg={0:.3f}, std={1:.3f}'.format(avg0, np.nanstd(flow_germ)))\n",
        "axs[0].grid()\n",
        "axs[1].title.set_text('Full Bloom - Germination, avg={0:.3f}, std={1:.3f}'.format(avg1, np.nanstd(fullb_germ)))\n",
        "axs[1].grid()\n",
        "axs[2].title.set_text('Full Bloom - Flowering, avg={0:.3f}, std={1:.3f}'.format(avg2, np.nanstd(fullb_flow)))\n",
        "axs[2].grid()\n",
        "\n",
        "fig.text(0.5, 0.1, 'Year', ha = 'center')\n",
        "fig.text(0.05, 0.5, 'Date', va='center', rotation='vertical')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1uTYzULRKVt"
      },
      "source": [
        "df_stations = df_stations.drop(stations_not_counted)\n",
        "\n",
        "df_stations['lat'] = gps_array[:,1]\n",
        "df_stations['lon'] = gps_array[:,2]\n",
        "\n",
        "df_stations.to_csv(path + 'stations_info.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YakIPd8lXRwl"
      },
      "source": [
        "# Calculation of GDD (Growing Degree-Days) Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9jpVLWidnSE"
      },
      "source": [
        "def calculate_gdd_year(year, stnId, endday, critical_temp):\n",
        "    if endday > 0:\n",
        "        temp_data = data_dict[str(stnId)][:,[0,3,4]]\n",
        "        gdd = 0\n",
        "        for i in range(len(temp_data[:,0])):\n",
        "            if temp_data[i,0] == year:\n",
        "                if temp_data[i, 1] < endday:\n",
        "                    gdd += max(temp_data[i,2] - critical_temp, 0)\n",
        "    else:\n",
        "        gdd = np.nan\n",
        "    return gdd\n",
        "\n",
        "def calculate_chill_day(year, stnId, endday, critical_temp):\n",
        "    if endday > 0:\n",
        "        temp_data = data_dict[str(stnId)][:,[0,3,4]]\n",
        "        count = 0\n",
        "        year_change = 0\n",
        "        for i in range(len(temp_data[:,0])):\n",
        "            if temp_data[i,0] == year - 1:\n",
        "                if temp_data[i,1] >= 300:\n",
        "                    year_change = 1\n",
        "                    if temp_data[i,2] <= critical_temp:\n",
        "                        count += 1\n",
        "            elif temp_data[i,0] == year and year_change == 1:\n",
        "                if temp_data[i,1] < endday:\n",
        "                    if temp_data[i,2] <= critical_temp:\n",
        "                        count += 1\n",
        "    else:\n",
        "        count = np.nan\n",
        "    return count\n",
        "\n",
        "def calculate_chill_gdd(year, stnId, endday, critical_temp):\n",
        "    if endday >= 0:\n",
        "        temp_data = data_dict[str(stnId)][:,[0,3,4]]\n",
        "        gdd = 0\n",
        "        year_change = 0\n",
        "        for i in range(len(temp_data[:,0])):\n",
        "            if temp_data[i,0] == year - 1:\n",
        "                if temp_data[i,1] >= 300:\n",
        "                    year_change = 1\n",
        "                    gdd += min(0, temp_data[i,2] - critical_temp)\n",
        "            elif temp_data[i,0] == year and year_change == 1:\n",
        "                if temp_data[i,1] < endday:\n",
        "                    gdd += min(0, temp_data[i,2] - critical_temp)\n",
        "    else:\n",
        "        gdd = np.nan\n",
        "    return gdd\n",
        "            \n",
        "\n",
        "def get_day(ftype, stnId, year):\n",
        "    if ftype == 'g':\n",
        "        for i, y in enumerate(germ_array[:,0]):\n",
        "            if y == year:\n",
        "                for j, code in enumerate(u_germ_codes):\n",
        "                    if int(stnId) == code:\n",
        "                        day = germ_array[i, j+1]\n",
        "                        return day\n",
        "    elif ftype == 'f':\n",
        "        for i, y in enumerate(flow_array[:,0]):\n",
        "            if y == year:\n",
        "                for j, code in enumerate(u_flow_codes):\n",
        "                    if int(stnId) == code:\n",
        "                        day = flow_array[i, j+1]\n",
        "                        return day\n",
        "    elif ftype == 'fb':\n",
        "        for i, y in enumerate(fullb_array[:,0]):\n",
        "            if y == year:\n",
        "                for j, code in enumerate(u_fullb_codes):\n",
        "                    if int(stnId) == code:\n",
        "                        day = fullb_array[i, j+1]\n",
        "                        return day\n",
        "    else:\n",
        "        print('invalid type code; try g, f, fb')\n",
        "\n",
        "def calculate_gdd(ftype, stnId, critical_temp):\n",
        "    temp_data = data_dict[str(stnId)][:,0]\n",
        "    temp_start_year = temp_data[0]\n",
        "    temp_end_year = temp_data[-1]\n",
        "    if ftype == 'g':\n",
        "        date_data = germ_array[:,0]\n",
        "    elif ftype == 'f':\n",
        "        date_data = flow_array[:,0]\n",
        "    elif ftype == 'fb':\n",
        "        date_data = fullb_array[:,0]\n",
        "    else:\n",
        "        print('invalid type code; try g, f, fb')\n",
        "    date_start_year = date_data[0]\n",
        "    date_end_year = date_data[-1]\n",
        "    start_year = int(max(temp_start_year, date_start_year))\n",
        "    end_year = int(min(temp_end_year, date_end_year))\n",
        "    gdd = []\n",
        "    dates = []\n",
        "    stn = []\n",
        "    years = range(start_year, end_year + 1)\n",
        "    for year in years:\n",
        "        day = get_day(ftype, stnId, year)\n",
        "        if day != 0:\n",
        "            gdd.append(calculate_gdd_year(year, stnId, day, critical_temp))\n",
        "            dates.append(day)\n",
        "            stn.append(stnId)\n",
        "        else:\n",
        "            gdd.append(np.nan)\n",
        "            dates.append(np.nan)\n",
        "            stn.append(stnId)\n",
        "    gdd = np.array(gdd).reshape(-1,1)\n",
        "    dates = np.array(dates).reshape(-1,1)\n",
        "    years = np.array(years).reshape(-1,1)\n",
        "    stn = np.array(stn).reshape(-1,1)\n",
        "    return np.hstack((stn, years, dates, gdd))\n",
        "    # stationID, year, budding date, GDD sum\n",
        "\n",
        "def get_criticaldates(ftype, stnId):\n",
        "    critical_temp = 5\n",
        "\n",
        "    temp_data = data_dict[str(stnId)][:,0]\n",
        "    temp_start_year = temp_data[0]\n",
        "    temp_end_year = temp_data[-1]\n",
        "    if ftype == 'g':\n",
        "        date_data = germ_array[:,0]\n",
        "    elif ftype == 'f':\n",
        "        date_data = flow_array[:,0]\n",
        "    elif ftype == 'fb':\n",
        "        date_data = fullb_array[:,0]\n",
        "    else:\n",
        "        print('invalid type code; try g, f, fb')\n",
        "    date_start_year = date_data[0]\n",
        "    date_end_year = date_data[-1]\n",
        "    start_year = int(max(temp_start_year, date_start_year))\n",
        "    end_year = int(min(temp_end_year, date_end_year))\n",
        "    years = range(start_year, end_year + 1)\n",
        "\n",
        "    critical_dates = []\n",
        "    temp_gdd_array = calculate_gdd(ftype, stnId, critical_temp)\n",
        "    avg = 0\n",
        "    for i in temp_gdd_array[:,3]:\n",
        "        if i > 0:\n",
        "            avg += i\n",
        "    avg = avg / len(temp_gdd_array[:,3])\n",
        "    for year in years:\n",
        "        for j in range(150):\n",
        "            temp = calculate_gdd_year(year, 108, j, critical_temp)\n",
        "            if temp >= avg:\n",
        "                critical_dates.append(j)\n",
        "                break\n",
        "    np_crit = np.array(critical_dates).reshape(-1,1)\n",
        "    return np.hstack((temp_gdd_array, np_crit))\n",
        "    # stationID, year, budding date, (heating) GDD sum, (heating) critical date"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-g9jUOpiY2v"
      },
      "source": [
        "# seoul_germ_array = stnID, year, budding date, (heating) GDD sum_budding, (heating) critical date_budding, flowering date\n",
        "\n",
        "seoul_germ_array = get_criticaldates('g', 108)\n",
        "seoul_flow_array = get_criticaldates('f', 108)\n",
        "\n",
        "ffd_dates = []\n",
        "for year in range(1961, 2021):\n",
        "    day = get_day('f', 108, year)\n",
        "    if day != 0:\n",
        "        ffd_dates.append(day)\n",
        "    else:\n",
        "        ffd_dates.append(np.nan)\n",
        "\n",
        "np_ffd = np.array(ffd_dates).reshape(-1, 1)\n",
        "seoul_germ_array = np.hstack((seoul_germ_array, np_ffd))\n",
        "\n",
        "df_seoul_germ = pd.DataFrame(seoul_germ_array[:,(0,1,2,5)], columns=['stnId', 'year', 'budding date', 'flowering date'], index=None)\n",
        "df_seoul_germ.to_csv(path + 'seoul_data.csv')\n",
        "\n",
        "df_seoul_flow = pd.DataFrame(seoul_flow_array[:,(0,2,4)], columns=['year', 'flowering date', 'critical date'], index=None)\n",
        "df_seoul_flow.to_csv(path + 'flowering_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrrw9EN4kUCv"
      },
      "source": [
        "## Chill Days Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be0HiP5ZZQz0"
      },
      "source": [
        "seoul_chill = []\n",
        "for i, year in enumerate(seoul_germ_array[:,1]):\n",
        "    critical_temp = 3.34\n",
        "    budding_day = get_day('g', 108, year)\n",
        "    chill_days = calculate_chill_day(year, 108, budding_day, critical_temp)\n",
        "    if chill_days > 0:\n",
        "        seoul_chill.append(chill_days)\n",
        "    else:\n",
        "        seoul_chill.append(np.nan)\n",
        "\n",
        "avg_chill = 0\n",
        "count = 0\n",
        "for i in seoul_chill:\n",
        "    if i > 0:\n",
        "        count += 1\n",
        "        avg_chill += i\n",
        "avg_chill = avg_chill / count\n",
        "\n",
        "################################################################################\n",
        "\n",
        "seoul_chill_gdd = []\n",
        "for i, year in enumerate(seoul_germ_array[:,1]):\n",
        "    critical_temp = 3.34\n",
        "    budding_day = get_day('g', 108, year)\n",
        "    chill_days_gdd = calculate_chill_gdd(year, 108, budding_day, critical_temp)\n",
        "    if chill_days_gdd >= 0 or chill_days_gdd < 0:\n",
        "        seoul_chill_gdd.append(chill_days_gdd)\n",
        "    else:\n",
        "        seoul_chill_gdd.append(np.nan)\n",
        "\n",
        "print(seoul_chill_gdd)\n",
        "\n",
        "\n",
        "avg_chill_gdd = 0\n",
        "count = 0\n",
        "for i in seoul_chill_gdd:\n",
        "    if i >= 0 or i < 0:\n",
        "        count += 1\n",
        "        avg_chill_gdd += i\n",
        "avg_chill_gdd = avg_chill_gdd / count\n",
        "print('average GDD : {}'.format(avg_chill_gdd))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efMwPjhurML0"
      },
      "source": [
        "critical_temp = 3.34\n",
        "years = []\n",
        "chill_critical_dates_gdd = []\n",
        "\n",
        "for i, year in enumerate(seoul_germ_array[:,1]):\n",
        "    for j in range(200):\n",
        "        chill_days_gdd = calculate_chill_gdd(year, 108, j, critical_temp)\n",
        "        if chill_days_gdd <= -450:\n",
        "            chill_critical_dates_gdd.append(j)\n",
        "            break\n",
        "        elif j == 199:\n",
        "            chill_critical_dates_gdd.append(np.nan)\n",
        "            break\n",
        "\n",
        "avg = [avg_chill_gdd] * len(seoul_germ_array[:,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A04D7UXjdhCR"
      },
      "source": [
        "# critical date 에 대해\n",
        "seoul_gdd_chill = []\n",
        "for i, crit in enumerate(chill_critical_dates_gdd):    \n",
        "    year = seoul_germ_array[i, 1]\n",
        "    gdd = 0\n",
        "    for j, y in enumerate(data_dict['108'][:,0]):\n",
        "        if y == year:\n",
        "            if data_dict['108'][j,3] >= crit and data_dict['108'][j,3] < seoul_germ_array[i, 5]:\n",
        "                gdd += max(0, data_dict['108'][j,4] - 1)\n",
        "    seoul_gdd_chill.append(gdd)\n",
        "\n",
        "print(seoul_gdd_chill)\n",
        "\n",
        "avg_gdd = 0\n",
        "count = 0\n",
        "for i in seoul_gdd_chill:\n",
        "    if i > 0:\n",
        "        count += 1\n",
        "    avg_gdd += i\n",
        "avg_gdd = avg_gdd / count\n",
        "\n",
        "# budding - flowering을 전부 chill daydates 합으로 계산한 경우 (seoul_flow_crit)\n",
        "seoul_flow_crit_chill = []\n",
        "for i, year in enumerate(seoul_germ_array[:,1]):\n",
        "    gdd = 0\n",
        "    crit = chill_critical_dates_gdd[i]\n",
        "    for j, y in enumerate(data_dict['108'][:,0]):\n",
        "        if y == year:\n",
        "            if data_dict['108'][j,3] >= crit:\n",
        "                gdd += max(0, data_dict['108'][j,4] - 1)\n",
        "            if gdd >= avg_gdd:\n",
        "                seoul_flow_crit_chill.append(data_dict['108'][j,3])\n",
        "                break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPcjOePKXKhW"
      },
      "source": [
        "rmse = 0\n",
        "count = 0\n",
        "for i, crit in enumerate(seoul_flow_crit_chill):\n",
        "    if crit > 0:\n",
        "        if seoul_germ_array[i+1,5] > 0:\n",
        "            rmse += (crit - seoul_germ_array[i+1,5]) ** 2\n",
        "            count += 1\n",
        "rmse = np.sqrt(rmse / count)\n",
        "print('발아를 거쳤을 때 개화 시기 RMSE : {}'.format(rmse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSkg_Pokkfkq"
      },
      "source": [
        "## GDD Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfdkBZTmqdoZ"
      },
      "source": [
        "# critical date 에 대해\n",
        "seoul_gdd = []\n",
        "for i, crit in enumerate(seoul_germ_array[:,4]):    \n",
        "    year = seoul_germ_array[i,1]\n",
        "    gdd = 0\n",
        "    for j, y in enumerate(data_dict['108'][:,0]):\n",
        "        if y == year:\n",
        "            if data_dict['108'][j,3] >= crit and data_dict['108'][j,3] < seoul_germ_array[i, 5]:\n",
        "                gdd += data_dict['108'][j,4]\n",
        "    seoul_gdd.append(gdd)\n",
        "\n",
        "avg_gdd = 0\n",
        "count = 0\n",
        "for i in seoul_gdd:\n",
        "    if i > 0:\n",
        "        count += 1\n",
        "    avg_gdd += i\n",
        "avg_gdd = avg_gdd / count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O8_c6Cm7msH"
      },
      "source": [
        "# budding - flowering을 전부 heat daydates 합으로 계산한 경우 (seoul_flow_crit)\n",
        "seoul_flow_crit = []\n",
        "for i, year in enumerate(seoul_germ_array[:,1]):\n",
        "    gdd = 0\n",
        "    crit = seoul_germ_array[i,4]\n",
        "    for j, y in enumerate(data_dict['108'][:,0]):\n",
        "        if y == year:\n",
        "            if data_dict['108'][j,3] >= crit:\n",
        "                gdd += max(0, data_dict['108'][j,4] - 1)\n",
        "            if gdd >= avg_gdd:\n",
        "                seoul_flow_crit.append(data_dict['108'][j,3])\n",
        "                break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW0zVpEp_hsY"
      },
      "source": [
        "rmse = 0\n",
        "count = 0\n",
        "for i, crit in enumerate(seoul_flow_crit):\n",
        "    if crit > 0:\n",
        "        if seoul_germ_array[i, 5] > 0:\n",
        "            rmse += (crit - seoul_germ_array[i, 5]) ** 2\n",
        "            count += 1\n",
        "rmse = np.sqrt(rmse / count)\n",
        "print('발아를 거쳤을 때 개화 시기 RMSE : {}'.format(rmse))\n",
        "\n",
        "rmse = 0\n",
        "count = 0\n",
        "for i, crit in enumerate(seoul_flow_array[:,4]):\n",
        "    if crit > 0:\n",
        "        if seoul_germ_array[i, 5] > 0:\n",
        "            rmse += (crit - seoul_flow_array[i,2]) ** 2\n",
        "            count += 1\n",
        "rmse = np.sqrt(rmse / count)\n",
        "print('발아를 거치지 않았을 때 개화 시기 RMSE : {}'.format(rmse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytJrCM3k1kyb"
      },
      "source": [
        "## Runtime 끊김 방지 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WfaA3u_1hGP"
      },
      "source": [
        "i = 1\n",
        "while i > 0:\n",
        "    i += 1\n",
        "    i -= 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}